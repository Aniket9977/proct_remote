{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened\")\n",
    "    exit()\n",
    "\n",
    "# Load pre-trained models for face and eye detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def get_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(np.array([eye_points[1].x, eye_points[1].y]) - np.array([eye_points[5].x, eye_points[5].y]))\n",
    "    B = np.linalg.norm(np.array([eye_points[2].x, eye_points[2].y]) - np.array([eye_points[4].x, eye_points[4].y]))\n",
    "    C = np.linalg.norm(np.array([eye_points[0].x, eye_points[0].y]) - np.array([eye_points[3].x, eye_points[3].y]))\n",
    "    aspect_ratio = (A + B) / (2.0 * C)\n",
    "    return aspect_ratio\n",
    "\n",
    "def get_mouth_aspect_ratio(mouth_points):\n",
    "    A = np.linalg.norm(np.array([mouth_points[13].x, mouth_points[13].y]) - np.array([mouth_points[19].x, mouth_points[19].y]))\n",
    "    B = np.linalg.norm(np.array([mouth_points[14].x, mouth_points[14].y]) - np.array([mouth_points[18].x, mouth_points[18].y]))\n",
    "    C = np.linalg.norm(np.array([mouth_points[12].x, mouth_points[12].y]) - np.array([mouth_points[16].x, mouth_points[16].y]))\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "def draw_face_landmarks(frame, landmarks):\n",
    "    for (x, y) in [(pt.x, pt.y) for pt in landmarks.parts()]:\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# Thresholds for EAR and MAR\n",
    "EAR_THRESHOLD = 0.25  # Adjusted threshold for eye aspect ratio\n",
    "MAR_THRESHOLD = 0.7   # Adjusted threshold for mouth aspect ratio\n",
    "\n",
    "# Variables to track suspicious activity duration\n",
    "suspicious_eye_start = None\n",
    "suspicious_mouth_start = None\n",
    "SUSPICIOUS_DURATION = 4  # 4 seconds\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Count the number of faces (people) detected\n",
    "    num_people = len(faces)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract eye coordinates\n",
    "        left_eye_points = [landmarks.part(i) for i in range(36, 42)]\n",
    "        right_eye_points = [landmarks.part(i) for i in range(42, 48)]\n",
    "\n",
    "        # Extract mouth coordinates\n",
    "        mouth_points = [landmarks.part(i) for i in range(48, 68)]\n",
    "\n",
    "        # Calculate EAR for both eyes\n",
    "        left_ear = get_aspect_ratio(left_eye_points)\n",
    "        right_ear = get_aspect_ratio(right_eye_points)\n",
    "\n",
    "        # Calculate MAR for the mouth\n",
    "        mar = get_mouth_aspect_ratio(mouth_points)\n",
    "\n",
    "        # Draw face landmarks\n",
    "        frame = draw_face_landmarks(frame, landmarks)\n",
    "\n",
    "        # Check if the EAR is below a certain threshold (for suspicious eye activity)\n",
    "        if left_ear < EAR_THRESHOLD or right_ear < EAR_THRESHOLD:\n",
    "            if suspicious_eye_start is None:\n",
    "                suspicious_eye_start = time.time()  # Start timing\n",
    "            elif time.time() - suspicious_eye_start >= SUSPICIOUS_DURATION:\n",
    "                timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "                cv2.putText(frame, f\"Suspicious Eye Activity! {timestamp}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            suspicious_eye_start = None  # Reset timer if no suspicious activity\n",
    "\n",
    "        # Check if the MAR is above a certain threshold (for suspicious mouth activity)\n",
    "        if mar > MAR_THRESHOLD:\n",
    "            if suspicious_mouth_start is None:\n",
    "                suspicious_mouth_start = time.time()  # Start timing\n",
    "            elif time.time() - suspicious_mouth_start >= SUSPICIOUS_DURATION:\n",
    "                timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "                cv2.putText(frame, f\"Suspicious Mouth Activity! {timestamp}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            suspicious_mouth_start = None  # Reset timer if no suspicious activity\n",
    "\n",
    "    # Display the number of people detected on the screen\n",
    "    cv2.putText(frame, f\"People Count: {num_people}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Remote Proctoring\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load pre-trained models for face and eye detection\u001b[39;00m\n\u001b[0;32m      9\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m---> 10\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_aspect_ratio\u001b[39m(eye_points):\n\u001b[0;32m     13\u001b[0m     A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(np\u001b[38;5;241m.\u001b[39marray([eye_points[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mx, eye_points[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39my]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray([eye_points[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mx, eye_points[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39my]))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened\")\n",
    "    exit()\n",
    "\n",
    "# Load pre-trained models for face and eye detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def get_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(np.array([eye_points[1].x, eye_points[1].y]) - np.array([eye_points[5].x, eye_points[5].y]))\n",
    "    B = np.linalg.norm(np.array([eye_points[2].x, eye_points[2].y]) - np.array([eye_points[4].x, eye_points[4].y]))\n",
    "    C = np.linalg.norm(np.array([eye_points[0].x, eye_points[0].y]) - np.array([eye_points[3].x, eye_points[3].y]))\n",
    "    aspect_ratio = (A + B) / (2.0 * C)\n",
    "    return aspect_ratio\n",
    "\n",
    "def get_mouth_aspect_ratio(mouth_points):\n",
    "    A = np.linalg.norm(np.array([mouth_points[13].x, mouth_points[13].y]) - np.array([mouth_points[19].x, mouth_points[19].y]))\n",
    "    B = np.linalg.norm(np.array([mouth_points[14].x, mouth_points[14].y]) - np.array([mouth_points[18].x, mouth_points[18].y]))\n",
    "    C = np.linalg.norm(np.array([mouth_points[12].x, mouth_points[12].y]) - np.array([mouth_points[16].x, mouth_points[16].y]))\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "def draw_face_landmarks(frame, landmarks):\n",
    "    for (x, y) in [(pt.x, pt.y) for pt in landmarks.parts()]:\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# Thresholds for EAR and MAR\n",
    "EAR_THRESHOLD = 0.18  # Adjusted threshold for eye aspect ratio\n",
    "MAR_THRESHOLD = 0.1   # Adjusted threshold for mouth aspect ratio\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Count the number of faces (people) detected\n",
    "    num_people = len(faces)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract eye coordinates\n",
    "        left_eye_points = [landmarks.part(i) for i in range(36, 42)]\n",
    "        right_eye_points = [landmarks.part(i) for i in range(42, 48)]\n",
    "\n",
    "        # Extract mouth coordinates\n",
    "        mouth_points = [landmarks.part(i) for i in range(48, 68)]\n",
    "\n",
    "        # Calculate EAR for both eyes\n",
    "        left_ear = get_aspect_ratio(left_eye_points)\n",
    "        right_ear = get_aspect_ratio(right_eye_points)\n",
    "\n",
    "        # Calculate MAR for the mouth\n",
    "        mar = get_mouth_aspect_ratio(mouth_points)\n",
    "\n",
    "        # Draw face landmarks\n",
    "        frame = draw_face_landmarks(frame, landmarks)\n",
    "\n",
    "        # Check if the EAR is below a certain threshold (for suspicious eye activity)\n",
    "        if left_ear < EAR_THRESHOLD or right_ear < EAR_THRESHOLD:\n",
    "            cv2.putText(frame, \"Suspicious Eye Activity!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"All Good!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Check if the MAR is above a certain threshold (for suspicious mouth activity)\n",
    "        if mar > MAR_THRESHOLD:\n",
    "            cv2.putText(frame, \"Suspicious Mouth Activity!\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"All Good!\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Display the number of people detected on the screen\n",
    "    cv2.putText(frame, f\"People Count: {num_people}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Remote Proctoring\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "# Streamlit page configuration\n",
    "st.set_page_config(page_title=\"Remote Proctoring\", layout=\"wide\")\n",
    "\n",
    "# Load pre-trained models for face and eye detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def get_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(np.array([eye_points[1].x, eye_points[1].y]) - np.array([eye_points[5].x, eye_points[5].y]))\n",
    "    B = np.linalg.norm(np.array([eye_points[2].x, eye_points[2].y]) - np.array([eye_points[4].x, eye_points[4].y]))\n",
    "    C = np.linalg.norm(np.array([eye_points[0].x, eye_points[0].y]) - np.array([eye_points[3].x, eye_points[3].y]))\n",
    "    aspect_ratio = (A + B) / (2.0 * C)\n",
    "    return aspect_ratio\n",
    "\n",
    "def get_mouth_aspect_ratio(mouth_points):\n",
    "    A = np.linalg.norm(np.array([mouth_points[13].x, mouth_points[13].y]) - np.array([mouth_points[19].x, mouth_points[19].y]))\n",
    "    B = np.linalg.norm(np.array([mouth_points[14].x, mouth_points[14].y]) - np.array([mouth_points[18].x, mouth_points[18].y]))\n",
    "    C = np.linalg.norm(np.array([mouth_points[12].x, mouth_points[12].y]) - np.array([mouth_points[16].x, mouth_points[16].y]))\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "def draw_face_landmarks(frame, landmarks):\n",
    "    for (x, y) in [(pt.x, pt.y) for pt in landmarks.parts()]:\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# Thresholds for EAR and MAR\n",
    "EAR_THRESHOLD = 0.25  # Adjusted threshold for eye aspect ratio\n",
    "MAR_THRESHOLD = 0.7   # Adjusted threshold for mouth aspect ratio\n",
    "\n",
    "# Streamlit application interface\n",
    "st.title(\"Remote Proctoring System\")\n",
    "st.write(\"This application detects suspicious eye and mouth activities, and counts the number of people in the frame.\")\n",
    "\n",
    "# Capture video frames\n",
    "run = st.checkbox('Run Camera')\n",
    "\n",
    "if run:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        st.error(\"Error: Camera could not be opened\")\n",
    "    else:\n",
    "        frame_placeholder = st.empty()\n",
    "        while run:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                st.error(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "\n",
    "            # Count the number of faces (people) detected\n",
    "            num_people = len(faces)\n",
    "\n",
    "            for face in faces:\n",
    "                landmarks = predictor(gray, face)\n",
    "\n",
    "                # Extract eye coordinates\n",
    "                left_eye_points = [landmarks.part(i) for i in range(36, 42)]\n",
    "                right_eye_points = [landmarks.part(i) for i in range(42, 48)]\n",
    "\n",
    "                # Extract mouth coordinates\n",
    "                mouth_points = [landmarks.part(i) for i in range(48, 68)]\n",
    "\n",
    "                # Calculate EAR for both eyes\n",
    "                left_ear = get_aspect_ratio(left_eye_points)\n",
    "                right_ear = get_aspect_ratio(right_eye_points)\n",
    "\n",
    "                # Calculate MAR for the mouth\n",
    "                mar = get_mouth_aspect_ratio(mouth_points)\n",
    "\n",
    "                # Draw face landmarks\n",
    "                frame = draw_face_landmarks(frame, landmarks)\n",
    "\n",
    "                # Check if the EAR is below a certain threshold (for suspicious eye activity)\n",
    "                if left_ear < EAR_THRESHOLD or right_ear < EAR_THRESHOLD:\n",
    "                    cv2.putText(frame, \"Suspicious Eye Activity!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                # Check if the MAR is above a certain threshold (for suspicious mouth activity)\n",
    "                if mar > MAR_THRESHOLD:\n",
    "                    cv2.putText(frame, \"Suspicious Mouth Activity!\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # Display the number of people detected on the screen\n",
    "            cv2.putText(frame, f\"People Count: {num_people}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "            # Convert frame to RGB format (Streamlit expects RGB images)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display the frame using Streamlit\n",
    "            frame_placeholder.image(frame, channels=\"RGB\")\n",
    "\n",
    "        cap.release()\n",
    "else:\n",
    "    st.write(\"Camera is not running. Check the 'Run Camera' checkbox to start.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\anike\\AppData\\Local\\Temp\\ipykernel_7164\\3952916312.py:47: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  log_dir = \"logs\\curr_logs\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file opened successfully at logs\\curr_logs\\suspicious_activity_log.txt.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Logged suspicious Mouth activity.\n",
      "Log file closed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened\")\n",
    "    exit()\n",
    "\n",
    "# Load pre-trained models for face and eye detection\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def get_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(np.array([eye_points[1].x, eye_points[1].y]) - np.array([eye_points[5].x, eye_points[5].y]))\n",
    "    B = np.linalg.norm(np.array([eye_points[2].x, eye_points[2].y]) - np.array([eye_points[4].x, eye_points[4].y]))\n",
    "    C = np.linalg.norm(np.array([eye_points[0].x, eye_points[0].y]) - np.array([eye_points[3].x, eye_points[3].y]))\n",
    "    aspect_ratio = (A + B) / (2.0 * C)\n",
    "    return aspect_ratio\n",
    "\n",
    "def get_mouth_aspect_ratio(mouth_points):\n",
    "    A = np.linalg.norm(np.array([mouth_points[13].x, mouth_points[13].y]) - np.array([mouth_points[19].x, mouth_points[19].y]))\n",
    "    B = np.linalg.norm(np.array([mouth_points[14].x, mouth_points[14].y]) - np.array([mouth_points[18].x, mouth_points[18].y]))\n",
    "    C = np.linalg.norm(np.array([mouth_points[12].x, mouth_points[12].y]) - np.array([mouth_points[16].x, mouth_points[16].y]))\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "def draw_face_landmarks(frame, landmarks):\n",
    "    for (x, y) in [(pt.x, pt.y) for pt in landmarks.parts()]:\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# Thresholds for EAR and MAR\n",
    "EAR_THRESHOLD = 0.18  # Adjusted threshold for eye aspect ratio\n",
    "MAR_THRESHOLD = 0.1   # Adjusted threshold for mouth aspect ratio\n",
    "\n",
    "# Variables to track suspicious activity duration\n",
    "suspicious_eye_start = None\n",
    "suspicious_mouth_start = None\n",
    "SUSPICIOUS_DURATION = 4  # 4 seconds\n",
    "\n",
    "# Directory where the log file will be saved\n",
    "log_dir = \"logs\\curr_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Path for the log file\n",
    "log_path = os.path.join(log_dir, \"suspicious_activity_log.txt\")\n",
    "\n",
    "# Try to open a log file and catch any errors\n",
    "try:\n",
    "    log_file = open(log_path, \"a\")\n",
    "    print(f\"Log file opened successfully at {log_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error opening log file: {e}\")\n",
    "    exit()\n",
    "\n",
    "def log_suspicious_activity(activity_type):\n",
    "    try:\n",
    "        timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "        log_entry = f\"{timestamp} - Suspicious {activity_type} Activity Detected\\n\"\n",
    "        log_file.write(log_entry)\n",
    "        log_file.flush()  # Ensure the log is written immediately\n",
    "        print(f\"Logged suspicious {activity_type} activity.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log file: {e}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Count the number of faces (people) detected\n",
    "    num_people = len(faces)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Extract eye coordinates\n",
    "        left_eye_points = [landmarks.part(i) for i in range(36, 42)]\n",
    "        right_eye_points = [landmarks.part(i) for i in range(42, 48)]\n",
    "\n",
    "        # Extract mouth coordinates\n",
    "        mouth_points = [landmarks.part(i) for i in range(48, 68)]\n",
    "\n",
    "        # Calculate EAR for both eyes\n",
    "        left_ear = get_aspect_ratio(left_eye_points)\n",
    "        right_ear = get_aspect_ratio(right_eye_points)\n",
    "\n",
    "        # Calculate MAR for the mouth\n",
    "        mar = get_mouth_aspect_ratio(mouth_points)\n",
    "\n",
    "        # Draw face landmarks\n",
    "        frame = draw_face_landmarks(frame, landmarks)\n",
    "\n",
    "        # Check if the EAR is below a certain threshold (for suspicious eye activity)\n",
    "        if left_ear < EAR_THRESHOLD or right_ear < EAR_THRESHOLD:\n",
    "            if suspicious_eye_start is None:\n",
    "                suspicious_eye_start = time.time()  # Start timing\n",
    "            elif time.time() - suspicious_eye_start >= SUSPICIOUS_DURATION:\n",
    "                timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "                cv2.putText(frame, f\"Suspicious Eye Activity! {timestamp}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                log_suspicious_activity(\"Eye\")\n",
    "        else:\n",
    "            suspicious_eye_start = None  # Reset timer if no suspicious activity\n",
    "\n",
    "        # Check if the MAR is above a certain threshold (for suspicious mouth activity)\n",
    "        if mar > MAR_THRESHOLD:\n",
    "            if suspicious_mouth_start is None:\n",
    "                suspicious_mouth_start = time.time()  # Start timing\n",
    "            elif time.time() - suspicious_mouth_start >= SUSPICIOUS_DURATION:\n",
    "                timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "                cv2.putText(frame, f\"Suspicious Mouth Activity! {timestamp}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                log_suspicious_activity(\"Mouth\")\n",
    "        else:\n",
    "            suspicious_mouth_start = None  # Reset timer if no suspicious activity\n",
    "\n",
    "    # Display the number of people detected on the screen\n",
    "    cv2.putText(frame, f\"People Count: {num_people}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Remote Proctoring\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "try:\n",
    "    log_file.close()  # Close the log file when the program exits\n",
    "    print(\"Log file closed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error closing log file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
